{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.pipeline import EntityRuler\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Initiating Spacy english version\n",
    "nlp= English()\n",
    "## Instantiating Entity ruler and adding to nlp via pipe\n",
    "ruler= EntityRuler(nlp)\n",
    "nlp.add_pipe(ruler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Creating Entity pattern\n",
    "### To retrieve patterns from the text for NER\n",
    "class EntityPattern():\n",
    "    \n",
    "    def dash_split(self,sentence):\n",
    "        ##If pattern list has no value then split directly else loop the list and split each value\n",
    "        if len(self.split_pattern_list) ==0:\n",
    "            self.split_pattern_list=sentence.split('-')\n",
    "        else:\n",
    "            dash_list=[]\n",
    "            for word in self.split_pattern_list:\n",
    "                word_list= word.split('-')\n",
    "                for split_word in word_list:\n",
    "                    dash_list.append(split_word)\n",
    "            \n",
    "            self.split_pattern_list= dash_list\n",
    "                \n",
    "    \n",
    "    def slash_split(self,sentence):\n",
    "\n",
    "        if len(self.split_pattern_list) ==0:\n",
    "            \n",
    "            self.split_pattern_list=sentence.split('/')\n",
    "        else:\n",
    "            slash_list=[]\n",
    "            for word in self.split_pattern_list:\n",
    "                word_list= word.split('/')\n",
    "                for split_word in word_list:\n",
    "                    slash_list.append(split_word)\n",
    "            \n",
    "            self.split_pattern_list= slash_list\n",
    "    \n",
    "    def star_split(self,sentence):\n",
    "\n",
    "        if len(self.split_pattern_list) ==0:\n",
    "            \n",
    "            self.split_pattern_list=sentence.split('*')\n",
    "        else:\n",
    "            star_list=[]\n",
    "            for word in self.split_pattern_list:\n",
    "                word_list= word.split('*')\n",
    "                for split_word in word_list:\n",
    "                    star_list.append(split_word)\n",
    "            \n",
    "            self.split_pattern_list= star_list\n",
    "    \n",
    "    def space_split(self,sentence):\n",
    "        \n",
    "        if len(self.split_pattern_list) ==0:\n",
    "            \n",
    "            self.split_pattern_list=sentence.split()\n",
    "        else:\n",
    "            space_list=[]\n",
    "            for word in self.split_pattern_list:\n",
    "                word_list= word.split(' ')\n",
    "                for split_word in word_list:\n",
    "                    space_list.append(split_word)\n",
    "            \n",
    "            self.split_pattern_list= space_list\n",
    "    \n",
    "    def pattern_creation(self,sentence):\n",
    "        self.split_pattern_list=[]\n",
    "        #print(sentence)\n",
    "        #self.dash_split(sentence)\n",
    "        #print(sentence)\n",
    "        #print(self.split_pattern_list)\n",
    "        self.slash_split(sentence)\n",
    "        #print(sentence)\n",
    "        #(self.split_pattern_list)\n",
    "        self.star_split(sentence)\n",
    "        #print(sentence)\n",
    "        #print(self.split_pattern_list)\n",
    "        self.space_split(sentence)\n",
    "        #print(sentence)\n",
    "        #print(self.split_pattern_list)\n",
    "        \n",
    "        \n",
    "        patterns=[]\n",
    "        \n",
    "        for word in self.split_pattern_list:\n",
    "            lower_case= word.lower()\n",
    "            \n",
    "            ##Creating patterns\n",
    "            word_dict={'LOWER':lower_case}\n",
    "            punct_dict={'IS_PUNCT':True, 'OP':'*'}\n",
    "            \n",
    "            patterns.append(word_dict)\n",
    "            patterns.append(punct_dict)\n",
    "        \n",
    "        ##Removing last punctuation dict\n",
    "        patterns= patterns[:-1]\n",
    "        \n",
    "        entity_rule=[{'label':'Role', 'pattern':patterns}]\n",
    "        \n",
    "        return entity_rule\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Calling Entity Class\n",
    "entity= EntityPattern()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'Role', 'pattern': [{'LOWER': 'search-'}, {'IS_PUNCT': True, 'OP': '*'}, {'LOWER': 'compensation'}]}]\n"
     ]
    }
   ],
   "source": [
    "pattern= entity.pattern_creation(u\"Search- Compensation\")\n",
    "print(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruler.add_patterns(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search- Compensation-->Role-->\n"
     ]
    }
   ],
   "source": [
    "doc= nlp(u\"Hello I am Search- Compensation\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text +'-->' + ent.label_+'-->')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please pass file path name: cities.csv\n"
     ]
    }
   ],
   "source": [
    "##Pass the file path name as input\n",
    "path_name= input('Please pass file path name: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_names= pd.read_csv(path_name, names=['city'], header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abbeville</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adamsville</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Addison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alabaster</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         city\n",
       "0   Abbeville\n",
       "1  Adamsville\n",
       "2     Addison\n",
       "3       Adger\n",
       "4   Alabaster"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_names.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_text= city_names['city'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-31c2dcd36109>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpattern_text\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mpattern\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mentity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpattern_creation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mruler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_patterns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\pipeline\\entityruler.py\u001b[0m in \u001b[0;36madd_patterns\u001b[1;34m(self, patterns)\u001b[0m\n\u001b[0;32m    248\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE097\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatterns\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoken_patterns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatterns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatterns\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mphrase_patterns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mphrase_matcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatterns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for text in pattern_text:\n",
    "    pattern= entity.pattern_creation(text)\n",
    "    ruler.add_patterns(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'Role', 'pattern': [{'LOWER': 'abbeville'}]},\n",
       " {'label': 'Role', 'pattern': [{'LOWER': 'adamsville'}]},\n",
       " {'label': 'Role', 'pattern': [{'LOWER': 'addison'}]},\n",
       " {'label': 'Role', 'pattern': [{'LOWER': 'adger'}]},\n",
       " {'label': 'Role', 'pattern': [{'LOWER': 'alabaster'}]},\n",
       " {'label': 'Role', 'pattern': [{'LOWER': 'albertville'}]},\n",
       " {'label': 'Role',\n",
       "  'pattern': [{'LOWER': 'alexander'},\n",
       "   {'IS_PUNCT': True, 'OP': '*'},\n",
       "   {'LOWER': 'city'}]},\n",
       " {'label': 'Role', 'pattern': [{'LOWER': 'alexandria'}]},\n",
       " {'label': 'Role', 'pattern': [{'LOWER': 'allgood'}]},\n",
       " {'label': 'Role', 'pattern': [{'LOWER': 'alpine'}]},\n",
       " {'label': 'Role', 'pattern': [{'LOWER': 'andalusia'}]},\n",
       " {'label': 'Role', 'pattern': [{'LOWER': 'anniston'}]},\n",
       " {'label': 'Role', 'pattern': [{'LOWER': 'arab'}]},\n",
       " {'label': 'Role', 'pattern': [{'LOWER': 'arlington'}]},\n",
       " {'label': 'Role', 'pattern': [{'LOWER': 'ashfor'}]}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ruler.patterns[1:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpine-->Role\n"
     ]
    }
   ],
   "source": [
    "doc= nlp(u'i will be there in alpine tomorrow morning')\n",
    "for ent in doc.ents:\n",
    "    print(ent.text+'-->' + ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
